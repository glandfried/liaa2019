\documentclass[shownotes]{beamer}
\input{../../aux/diapo_encabezado.tex}

\title[Bayesian inference]{Evidence in favor of a scientific theory\\ \large With great complexity comes great honesty}

\author[Gustavo Landfried]{Gustavo Landfried \\ \scriptsize \texttt{@GALandfried} \\\vspace{0.2cm}
MSc in Anthropological Sciences \\
PhD student in Computer Sciences 
\vspace{-0.3cm}}
\institute[DC-ICC-CONICET]{\includegraphics[width=0.4\textwidth]{../../Imagenes/dc-logo}}
\date{}

\begin{document}
\begin{frame}[noframenumbering]
 
 \begin{textblock}{100}(23,18)
 \includegraphics[width=0.3\textwidth]{../../Imagenes/logo_licar} 
 \end{textblock}
  \begin{textblock}{100}(74,18)
 \includegraphics[width=0.3\textwidth]{../../Imagenes/LIAA/logo_version_02} 
 \end{textblock}

\vspace{2.5cm}
\maketitle
 
\end{frame}

\footnotesize


\begin{frame}
\begin{textblock}{128}(0,8)
\begin{center}
 \Large Why Bayesian inference?
\end{center}
\end{textblock}
\vspace{0.75cm}
 
 \Wider[0.3em]{
 \begin{mdframed}[backgroundcolor=black!15]
\centering \Large
Allows us to optimally update a priori beliefs given a model and data.
 \end{mdframed}
}
 
\end{frame}

\section{Conditional probability}

\begin{frame}
\begin{textblock}{128}(0,8)
\begin{center}
 \Large Where comes from? 
\end{center}
\end{textblock}
\vspace{0.75cm}


\begin{table}[H]
\begin{tabular}{c|c|c|c}
 & $A_1$ & $A_2$ &  \\ \hline
 $B_1$& 4 & 2 & 6 \\ \hline
 $B_2$& 76 & 18 & 94\\ \hline
 & 80 & 20 & 100
\end{tabular}
\end{table}

\begin{center}
\large From conditional probability
\end{center}

\pause

\begin{equation*}
 P(A_1|B_1) = \frac{P(B_1 \cap A_1)}{P(B_1)}
\end{equation*}

\pause 

\begin{mdframed}
Bayes theorem:
\begin{equation}
  P(A_1|B_1) = \frac{P(B_1|A_1)P(A_1)}{P(B_1)}
\end{equation}
\end{mdframed}

\end{frame}

\subsection{Scientific test example}

\begin{frame}
\begin{textblock}{128}(0,8)
\begin{center}
 \large Scientific test example
\end{center}
\end{textblock}
\vspace{0.5cm}


{ \footnotesize
 There is a test that correctly detects vampirism $95\%$ of the time.
 
 $\bullet$ $P(\text{positive}|\text{vampire})=0.95$
 
 \pause \medskip 

 One percent of the time it incorrectly detect normal personas as vampires.
 
 $\bullet$ $P(\text{positive}|\text{mortal})=0.01$
 
 \pause \medskip 
 
 We know that vampires are only $0.1\%$ of the population. 
 
 $\bullet$ $P(\text{vampire})=0.001$

}

\pause

\begin{center}
 Someone receive a positive test: 
 
 \pause
 She has \textbf{only 8.7\% chance} to actually be a vampire!?
\end{center}

\begin{equation*}
 P(\text{vampire}|\text{positive}) = \frac{P(\text{positive}|\text{vampire})P(\text{vampire})}{P(\text{positive})}
\end{equation*}

\pause


\begin{mdframed}[backgroundcolor=black!15]
 \centering \normalsize
 In this example all frequencies were observable
\end{mdframed}

\end{frame}

\subsection{The inferential jump}

\begin{frame}
\begin{textblock}{128}(0,8)
\begin{center}
 \Large The inferential jump
\end{center}
\end{textblock}

\begin{textblock}{128}(0,25)
 \centering 
  \textbf{Bayesian inference is about about hidden variables}

 About our \textbf{belief distributions} of those hidden variables!
\end{textblock}

\begin{textblock}{128}(0,40)
\only<2>{
\begin{equation*}
\underbrace{P(\text{Belief}|\text{Data})}_{\text{\scriptsize Posterior}} = \frac{\overbrace{P(\text{Data}|\text{Belief})}^{\text{\scriptsize Likelihood}} \overbrace{P(\text{Belief})}^{\text{\scriptsize Prior}} }{\underbrace{P(\text{Data})}_{\hfrac{\text{\scriptsize Evidence or}}{\text{\scriptsize Average likelihood}}}}
\end{equation*}
}

\only<3->{  
\begin{equation*}
\underbrace{P(\text{Belief}|\text{Data},\text{Model})}_{\text{\scriptsize Posterior}} = \frac{\overbrace{P(\text{Data}|\text{Belief},\text{Model})}^{\text{\scriptsize Likelihood}} \overbrace{P(\text{Belief}|\text{Model})}^{\text{\scriptsize Prior}} }{\underbrace{P(\text{Data}|\text{Model})}_{\hfrac{\text{\scriptsize Evidence or}}{\text{\scriptsize Average likelihood}}}}
 \end{equation*} 
}
\end{textblock}


\begin{textblock}{112}(8,70)
\only<4->{ 
To update our beliefs (posterior), we need to
\vspace{-0.15cm}
 \begin{mdframed}[backgroundcolor=black!15] 
 \centering \large 
  Consider every possible path in the model
  that could have lead as to the observed data (likelihood).
 \end{mdframed}
 }
\end{textblock}

 
 
\end{frame}

\section{The inferential jump}

\begin{frame}
 
 \footnotesize
 
$\bullet$ \textbf{Prior} belief (distribution):
 \begin{equation*}
P(B|M) = \frac{1}{\# \text{Beliefs} }  \ \ \ \ \  \forall B \in \text{Beliefs}  
 \end{equation*}

\pause \vspace{0.3cm}


 $\bullet$ \textbf{Likelihood} or average ways that produce data (distribution):
 \begin{equation*}
P(D|B,M) =  \frac{\text{Ways to produce $D$ given $B$ and $M$}}{\text{Total ways given $B$ and $M$}} \ \ \ \ \  \forall  B \in \text{Beliefs}  
 \end{equation*}

 
 %  \begin{equation*}
% P(D|B,M) =  \sum_{W}^{\hfrac{\text{Ways to produce $D$}}{\text{given $B$ and $M$}}} P(W|B,M) \ \ \ \ \  \forall  B \in \text{Beliefs}  
%  \end{equation*}

\pause \vspace{0.3cm}


$\bullet$ \textbf{Evidence} or Average likelihood (scalar):
\begin{equation*}
 P(D|M) = \sum_{B \in \text{Beliefs}} \underbrace{P(D|B,M)}_{\text{likelihood}} \underbrace{P(B|M)}_{\text{prior}}
\end{equation*}

\pause \vspace{0.3cm}

$\bullet$ \textbf{Posterior} belief (distribution):

\begin{equation*}
 P(B|D,M) = \frac{P(D|B,M)P(B|M)}{P(D|M)}\ \ \ \ \  \forall B \in \text{Beliefs}  
\end{equation*}

 
\end{frame}



\subsection{The garden of forking paths}


\begin{frame}
\begin{textblock}{128}(0,8)
\begin{center}
 \large The garden of forking paths
\end{center}
\end{textblock}
\vspace{0.5cm}

\begin{textblock}{128}(0,19)
\centering
 Data: \includegraphics[page=1,width=0.065\textwidth]{imagenes/forkingPath} \hspace{0.3cm}
 Beliefs: \includegraphics[page=2,width=0.075\textwidth]{imagenes/forkingPath}, \includegraphics[page=3,width=0.075\textwidth]{imagenes/forkingPath}, \includegraphics[page=4,width=0.075\textwidth]{imagenes/forkingPath}, \includegraphics[page=5,width=0.075\textwidth]{imagenes/forkingPath}, \includegraphics[page=6,width=0.075\textwidth]{imagenes/forkingPath} 

 \vspace{0.2cm}

  Model: Data $\sim \text{Binomial}(n,p)$\vspace{0.1cm}
 
\end{textblock}

\only<2>{
\begin{textblock}{128}(90,66)
 \scriptsize (First marbel)
\end{textblock}
\begin{textblock}{128}(0,30)
\end{textblock}

 \begin{textblock}{128}(0,30)
\centering
 

\includegraphics[page=12,width=0.6\textwidth]{imagenes/forkingPath}

\scriptsize Ways given $M$ and $B=$ \includegraphics[page=2,width=0.05\textwidth]{imagenes/forkingPath}

\end{textblock}
}


\only<3>{
\begin{textblock}{128}(90,66)
 \scriptsize (Second marbel)
\end{textblock}

\begin{textblock}{128}(0,30)
\centering
 

\includegraphics[page=13,width=0.6\textwidth]{imagenes/forkingPath}

\scriptsize Ways given $M$ and $B=$ \includegraphics[page=2,width=0.05\textwidth]{imagenes/forkingPath}

\end{textblock}
}

\only<4>{
\begin{textblock}{128}(90,66)
 \scriptsize (Second marbel)
\end{textblock}
\begin{textblock}{128}(0,30)
\centering
 

\includegraphics[page=14,width=0.6\textwidth]{imagenes/forkingPath}

\scriptsize Ways given $M$ and $B=$ \includegraphics[page=2,width=0.05\textwidth]{imagenes/forkingPath}

\end{textblock}
}

\only<5>{
\begin{textblock}{128}(90,66)
 \scriptsize (Third marbel)
\end{textblock}
\begin{textblock}{128}(0,30)
\centering
 

\includegraphics[page=15,width=0.6\textwidth]{imagenes/forkingPath}

\scriptsize Ways given $M$ and $B=$ \includegraphics[page=2,width=0.05\textwidth]{imagenes/forkingPath}

\end{textblock}
}

\only<6>{
\begin{textblock}{128}(90,66)
 \scriptsize (Third marbel)
\end{textblock}
 \begin{textblock}{128}(0,30)
\centering
 

\includegraphics[page=16,width=0.6\textwidth]{imagenes/forkingPath}

\scriptsize Ways given $M$ and $B=$ \includegraphics[page=2,width=0.05\textwidth]{imagenes/forkingPath}

\end{textblock}
}

\only<7>{
\begin{textblock}{128}(0,30)
\centering
 

\includegraphics[page=16,width=0.6\textwidth]{imagenes/forkingPath}

\scriptsize Ways given $M$ and $B=$ \includegraphics[page=2,width=0.05\textwidth]{imagenes/forkingPath}

\end{textblock}
}


\only<8>{
 \begin{textblock}{128}(0,30)
\centering
 

\includegraphics[page=7,width=0.6\textwidth]{imagenes/forkingPath}

\scriptsize Ways given $M$ and $B=$ \includegraphics[page=2,width=0.05\textwidth]{imagenes/forkingPath}
\end{textblock}
}


\only<9>{
 \begin{textblock}{128}(0,30)
\centering
 

\includegraphics[page=8,width=0.6\textwidth]{imagenes/forkingPath}

\scriptsize Ways given $M$ and $B=$ \includegraphics[page=3,width=0.05\textwidth]{imagenes/forkingPath}
\end{textblock}
}

\only<10>{
 \begin{textblock}{128}(0,30)
\centering
 

\includegraphics[page=9,width=0.6\textwidth]{imagenes/forkingPath}

\scriptsize Ways given $M$ and $B=$ \includegraphics[page=4,width=0.05\textwidth]{imagenes/forkingPath}
\end{textblock}
}

\only<11>{
 \begin{textblock}{128}(0,30)
\centering
 

\includegraphics[page=10,width=0.6\textwidth]{imagenes/forkingPath}

\scriptsize Ways given $M$ and $B=$ \includegraphics[page=5,width=0.05\textwidth]{imagenes/forkingPath}
\end{textblock}
}

\only<12->{
 \begin{textblock}{128}(0,30)
\centering
 

\includegraphics[page=11,width=0.6\textwidth]{imagenes/forkingPath}

\scriptsize Ways given $M$ and $B=$ \includegraphics[page=6,width=0.05\textwidth]{imagenes/forkingPath}
\end{textblock}
}





\only<7>{
\begin{textblock}{128}(0,67)
 \centering \scriptsize
 
 \begin{table}[H]
 \tiny 
\begin{tabular}{cccccc}
 \ \ \ Belief \ \ \ &  Ways to produce  \includegraphics[page=1,width=0.045\textwidth]{imagenes/forkingPath}  
& \textcolor{white}{Likelihood} & \textcolor{white}{Prior} & \textcolor{white}{Posterior $\propto$} & \textcolor{white}{Posterior}
\\ \cline{1-2} \\[-0.2cm]

\includegraphics[page=2,width=0.05\textwidth]{imagenes/forkingPath} &   & \textcolor{white}{$\frac{0 \times 4 \times 0 }{4 \times 4 \times 4 } = \frac{0}{64} $} &  &  & \textcolor{white}{$\frac{0}{3+8+9} =  0.00 $}
 \\[2pt]
  & & & & &\\[2pt]
  & & & & &\\[2pt]
  & & & & &\\[2pt]
  & & & & & \\[2pt]  \\[-0.2cm]
 & & & & &
\end{tabular}
\end{table}
\end{textblock}

}



\only<8>{
\begin{textblock}{128}(0,67)
 \centering \scriptsize
 
 \begin{table}[H]
 \tiny 
\begin{tabular}{cccccc}
\ \ \ Belief \ \ \ &  Ways to produce  \includegraphics[page=1,width=0.045\textwidth]{imagenes/forkingPath}  
& \textcolor{white}{Likelihood} & \textcolor{white}{Prior} & \textcolor{white}{Posterior $\propto$} & \textcolor{white}{Posterior}
\\ \cline{1-2} \\[-0.2cm]

\includegraphics[page=2,width=0.05\textwidth]{imagenes/forkingPath} & $0 \times 4 \times 0 = 0$  & \textcolor{white}{$\frac{0 \times 4 \times 0 }{4 \times 4 \times 4 } = \frac{0}{64} $} &  &  & \textcolor{white}{$\frac{0}{3+8+9} =  0.00 $}
 \\[2pt]
  & & & & &\\[2pt]
  & & & & &\\[2pt]
  & & & & &\\[2pt]
  & & & & & \\[2pt]  \\[-0.2cm]
 & & & & &
\end{tabular}
\end{table}
\end{textblock}

}

\only<9>{
\begin{textblock}{128}(0,67)
 \centering \scriptsize
 
 \begin{table}[H]
 \tiny 
\begin{tabular}{cccccc}
\ \ \ Belief \ \ \ &  Ways to produce \includegraphics[page=1,width=0.045\textwidth]{imagenes/forkingPath}  
&Likelihood&Prior&Posterior $\propto$& \textcolor{white}{Posterior}
\\ \cline{1-5} \\[-0.2cm]

  \includegraphics[page=2,width=0.05\textwidth]{imagenes/forkingPath} &  $0 \times 4 \times 0 = 0$ 
 & $\frac{0 \times 4 \times 0 }{4 \times 4 \times 4 } = \frac{0}{64} $  & $1/5$ & $\frac{0}{64}\frac{1}{5}$ & \textcolor{white}{$\frac{0}{3+8+9} =  0.00 $}
 \\[2pt]
  & & & & &\\[2pt]
  & & & & &\\[2pt]
  & & & & &\\[2pt]
  & & & & & \\[2pt]  \\[-0.2cm]
 & & & & &
\end{tabular}
\end{table}
\end{textblock}

}

\only<10>{
\begin{textblock}{128}(0,67)
 \centering \scriptsize
 
 \begin{table}[H]
 \tiny 
\begin{tabular}{cccccc}
\ \ \ Belief \ \ \ &  Ways to produce  \includegraphics[page=1,width=0.045\textwidth]{imagenes/forkingPath}  
& Likelihood & Prior & Posterior $\propto$ & \textcolor{white}{Posterior}
\\ \cline{1-5} \\[-0.2cm]

  \includegraphics[page=2,width=0.05\textwidth]{imagenes/forkingPath} &  $0 \times 4 \times 0 = 0$ 
 & $\frac{0 \times 4 \times 0 }{4 \times 4 \times 4 } = \frac{0}{64} $  & $1/5$ & $\frac{0}{64}\frac{1}{5}$ & \textcolor{white}{$\frac{0}{3+8+9} =  0.00 $}
 \\[2pt]
  \includegraphics[page=3,width=0.05\textwidth]{imagenes/forkingPath} &  $1 \times 3 \times 1 = 3$ 
 &$ 3/64 $ & $1/5$ & $\frac{3}{64}\frac{1}{5}$&\\[2pt]
  & & & & &\\[2pt]
  & & & & &\\[2pt]
  & & & & & \\[2pt]  \\[-0.2cm]
 & & & & &
\end{tabular}
\end{table}
\end{textblock}

}

\only<11>{
\begin{textblock}{128}(0,67)
 \centering \scriptsize
 
 \begin{table}[H]
 \tiny 
\begin{tabular}{cccccc}
\ \ \ Belief \ \ \ &  Ways to produce  \includegraphics[page=1,width=0.045\textwidth]{imagenes/forkingPath}  
& Likelihood & Prior & Posterior $\propto$ & \textcolor{white}{Posterior}
\\ \cline{1-5} \\[-0.2cm]

  \includegraphics[page=2,width=0.05\textwidth]{imagenes/forkingPath} &  $0 \times 4 \times 0 = 0$ 
 & $\frac{0 \times 4 \times 0 }{4 \times 4 \times 4 } = \frac{0}{64} $  & $1/5$ & $\frac{0}{64}\frac{1}{5}$ & \textcolor{white}{$\frac{0}{3+8+9} =  0.00 $}
 \\[2pt]
  \includegraphics[page=3,width=0.05\textwidth]{imagenes/forkingPath} &  $1 \times 3 \times 1 = 3$ 
 &$ 3/64 $ & $1/5$ & $\frac{3}{64}\frac{1}{5}$&\\[2pt]
  \includegraphics[page=4,width=0.05\textwidth]{imagenes/forkingPath} &  $2 \times 2 \times 2 = 8$ &$8/64$&$1/5$ &$\frac{8}{64}\frac{1}{5}$ &\\[2pt]
  & & & & &\\[2pt]
  & & & & & \\[2pt]  \\[-0.2cm]
 & & & & &
\end{tabular}
\end{table}
\end{textblock}

}

\only<12>{
\begin{textblock}{128}(0,67)
 \centering \scriptsize
 
 \begin{table}[H]
 \tiny 
\begin{tabular}{cccccc}
\ \ \ Belief \ \ \ &  Ways to produce  \includegraphics[page=1,width=0.045\textwidth]{imagenes/forkingPath}  
& Likelihood & Prior & Posterior $\propto$ & \textcolor{white}{Posterior}
\\ \cline{1-5} \\[-0.2cm]

  \includegraphics[page=2,width=0.05\textwidth]{imagenes/forkingPath} &  $0 \times 4 \times 0 = 0$ 
 & $\frac{0 \times 4 \times 0 }{4 \times 4 \times 4 } = \frac{0}{64} $  & $1/5$ & $\frac{0}{64}\frac{1}{5}$ & \textcolor{white}{$\frac{0}{3+8+9} =  0.00 $}
 \\[2pt]
  \includegraphics[page=3,width=0.05\textwidth]{imagenes/forkingPath} &  $1 \times 3 \times 1 = 3$ 
 &$ 3/64 $ & $1/5$ & $\frac{3}{64}\frac{1}{5}$&\\[2pt]
  \includegraphics[page=4,width=0.05\textwidth]{imagenes/forkingPath} &  $2 \times 2 \times 2 = 8$ &$8/64$&$1/5$ &$\frac{8}{64}\frac{1}{5}$ &\\[2pt]
  \includegraphics[page=5,width=0.05\textwidth]{imagenes/forkingPath} &  $3 \times 1 \times 3 = 9$ &$9/64$&$1/5$ &$\frac{9}{64}\frac{1}{5}$&\\[2pt]
  & & & & & \\[2pt]  \\[-0.2cm]
 & & & & &
\end{tabular}
\end{table}
\end{textblock}

}


\only<13>{
\begin{textblock}{128}(0,67)
 \centering \scriptsize
 
 \begin{table}[H]
 \tiny 
\begin{tabular}{cccccc}
\ \ \ Belief \ \ \ &  Ways to produce  \includegraphics[page=1,width=0.045\textwidth]{imagenes/forkingPath}  
& Likelihood & Prior & Posterior $\propto$ & \textcolor{white}{Posterior}
\\ \cline{1-5} \\[-0.2cm]

  \includegraphics[page=2,width=0.05\textwidth]{imagenes/forkingPath} &  $0 \times 4 \times 0 = 0$ 
 & $\frac{0 \times 4 \times 0 }{4 \times 4 \times 4 } = \frac{0}{64} $  & $1/5$ & $\frac{0}{64}\frac{1}{5}$ & \textcolor{white}{$\frac{0}{3+8+9} =  0.00 $}
 \\[2pt]
  \includegraphics[page=3,width=0.05\textwidth]{imagenes/forkingPath} &  $1 \times 3 \times 1 = 3$ 
 &$ 3/64 $ & $1/5$ & $\frac{3}{64}\frac{1}{5}$&\\[2pt]
  \includegraphics[page=4,width=0.05\textwidth]{imagenes/forkingPath} &  $2 \times 2 \times 2 = 8$ &$8/64$&$1/5$ &$\frac{8}{64}\frac{1}{5}$ &\\[2pt]
  \includegraphics[page=5,width=0.05\textwidth]{imagenes/forkingPath} &  $3 \times 1 \times 3 = 9$ &$9/64$&$1/5$ &$\frac{9}{64}\frac{1}{5}$&\\[2pt]
  \includegraphics[page=6,width=0.05\textwidth]{imagenes/forkingPath} &  $4 \times 0 \times 4 = 0$ &$0/64$&$1/5$ &$\frac{0}{64}\frac{1}{5}$&\\[2pt]  \\[-0.2cm]
 & & & & &
\end{tabular}
\end{table}
\end{textblock}

}


\only<14>{
\begin{textblock}{128}(0,67)
 \centering \scriptsize
 
 \begin{table}[H]
 \tiny 
\begin{tabular}{cccccc}
\ \ \ Belief \ \ \ &  Ways to produce  \includegraphics[page=1,width=0.045\textwidth]{imagenes/forkingPath}  
& Likelihood & Prior & Posterior $\propto$ & \textcolor{white}{Posterior}
\\ \cline{1-5} \\[-0.2cm]

  \includegraphics[page=2,width=0.05\textwidth]{imagenes/forkingPath} &  $0 \times 4 \times 0 = 0$ 
 & $\frac{0 \times 4 \times 0 }{4 \times 4 \times 4 } = \frac{0}{64} $  & $1/5$ & $\frac{0}{64}\frac{1}{5}$ & \textcolor{white}{$\frac{0}{3+8+9} =  0.00 $}
 \\[2pt]
  \includegraphics[page=3,width=0.05\textwidth]{imagenes/forkingPath} &  $1 \times 3 \times 1 = 3$ 
 &$ 3/64 $ & $1/5$ & $\frac{3}{64}\frac{1}{5}$&\\[2pt]
  \includegraphics[page=4,width=0.05\textwidth]{imagenes/forkingPath} &  $2 \times 2 \times 2 = 8$ &$8/64$&$1/5$ &$\frac{8}{64}\frac{1}{5}$ &\\[2pt]
  \includegraphics[page=5,width=0.05\textwidth]{imagenes/forkingPath} &  $3 \times 1 \times 3 = 9$ &$9/64$&$1/5$ &$\frac{9}{64}\frac{1}{5}$&\\[2pt]
  \includegraphics[page=6,width=0.05\textwidth]{imagenes/forkingPath} &  $4 \times 0 \times 4 = 0$ &$0/64$&$1/5$ &$\frac{0}{64}\frac{1}{5}$&\\[2pt]  \cline{5-5} \\[-0.2cm]
 & & & & $P(D|M)$ &
\end{tabular}
\end{table}
\end{textblock}

}



\only<15>{
\begin{textblock}{128}(0,67)
 \centering \scriptsize
 
 \begin{table}[H]
 \tiny 
\begin{tabular}{cccccc}
\ \ \ Belief \ \ \ &  Ways to produce  \includegraphics[page=1,width=0.045\textwidth]{imagenes/forkingPath}  
& Likelihood & Prior & Posterior $\propto$ & \textcolor{white}{Posterior}
\\ \cline{1-5} \\[-0.2cm]

  \includegraphics[page=2,width=0.05\textwidth]{imagenes/forkingPath} &  $0 \times 4 \times 0 = 0$ 
 & $\frac{0 \times 4 \times 0 }{4 \times 4 \times 4 } = \frac{0}{64} $  & $1/5$ & $\frac{0}{64}\frac{1}{5}$ & \textcolor{white}{$\frac{0}{3+8+9} =  0.00 $}
 \\[2pt]
  \includegraphics[page=3,width=0.05\textwidth]{imagenes/forkingPath} &  $1 \times 3 \times 1 = 3$ 
 &$ 3/64 $ & $1/5$ & $\frac{3}{64}\frac{1}{5}$&\\[2pt]
  \includegraphics[page=4,width=0.05\textwidth]{imagenes/forkingPath} &  $2 \times 2 \times 2 = 8$ &$8/64$&$1/5$ &$\frac{8}{64}\frac{1}{5}$ &\\[2pt]
  \includegraphics[page=5,width=0.05\textwidth]{imagenes/forkingPath} &  $3 \times 1 \times 3 = 9$ &$9/64$&$1/5$ &$\frac{9}{64}\frac{1}{5}$&\\[2pt]
  \includegraphics[page=6,width=0.05\textwidth]{imagenes/forkingPath} &  $4 \times 0 \times 4 = 0$ &$0/64$&$1/5$ &$\frac{0}{64}\frac{1}{5}$&\\[2pt] \cline{5-5} \\[-0.2cm]
 & & & & $\frac{3 + 8 + 9 }{64 \cdot 5} $&
\end{tabular}
\end{table}
\end{textblock}

}


\only<16>{
\begin{textblock}{128}(0,67)
 \centering \scriptsize
 
 \begin{table}[H]
 \tiny 
\begin{tabular}{cccccc}
\ \ \ Belief \ \ \ &  Ways to produce  \includegraphics[page=1,width=0.045\textwidth]{imagenes/forkingPath}  
& Likelihood & Prior & Posterior $\propto$ & Posterior
\\ \cline{1-6} \\[-0.2cm]

  \includegraphics[page=2,width=0.05\textwidth]{imagenes/forkingPath} &  $0 \times 4 \times 0 = 0$ 
 & $\frac{0 \times 4 \times 0 }{4 \times 4 \times 4 } = \frac{0}{64} $  & $1/5$ & $\frac{0}{64}\frac{1}{5}$ & $ \frac{0}{64}\frac{1}{5}  \frac{64\cdot 5}{3+8+9}$
 \\[2pt]
  \includegraphics[page=3,width=0.05\textwidth]{imagenes/forkingPath} &  $1 \times 3 \times 1 = 3$ 
 &$ 3/64 $ & $1/5$ & $\frac{3}{64}\frac{1}{5}$&\textcolor{white}{$\frac{3}{3+8+9} =  0.00 $}\\[2pt]
  \includegraphics[page=4,width=0.05\textwidth]{imagenes/forkingPath} &  $2 \times 2 \times 2 = 8$ &$8/64$&$1/5$ &$\frac{8}{64}\frac{1}{5}$ &\\[2pt]
  \includegraphics[page=5,width=0.05\textwidth]{imagenes/forkingPath} &  $3 \times 1 \times 3 = 9$ &$9/64$&$1/5$ &$\frac{9}{64}\frac{1}{5}$&\\[2pt]
  \includegraphics[page=6,width=0.05\textwidth]{imagenes/forkingPath} &  $4 \times 0 \times 4 = 0$ &$0/64$&$1/5$ &$\frac{0}{64}\frac{1}{5}$&\\[2pt] \cline{5-5} \\[-0.2cm]
 & & & & $\frac{3 + 8 + 9 }{64 \cdot 5} $&
\end{tabular}
\end{table}
\end{textblock}

}


\only<17>{
\begin{textblock}{128}(0,67)
 \centering \scriptsize
 
 \begin{table}[H]
 \tiny 
\begin{tabular}{cccccc}
\ \ \ Belief \ \ \ &  Ways to produce  \includegraphics[page=1,width=0.045\textwidth]{imagenes/forkingPath}  
& Likelihood & Prior & Posterior $\propto$ & Posterior
\\ \cline{1-6} \\[-0.2cm]

  \includegraphics[page=2,width=0.05\textwidth]{imagenes/forkingPath} &  $0 \times 4 \times 0 = 0$ 
 & $\frac{0 \times 4 \times 0 }{4 \times 4 \times 4 } = \frac{0}{64} $  & $1/5$ & $\frac{0}{64}\frac{1}{5}$ & $\frac{0}{3+8+9} =  0.00 $
 \\[2pt]
  \includegraphics[page=3,width=0.05\textwidth]{imagenes/forkingPath} &  $1 \times 3 \times 1 = 3$ 
 &$ 3/64 $ & $1/5$ & $\frac{3}{64}\frac{1}{5}$&\\[2pt]
  \includegraphics[page=4,width=0.05\textwidth]{imagenes/forkingPath} &  $2 \times 2 \times 2 = 8$ &$8/64$&$1/5$ &$\frac{8}{64}\frac{1}{5}$ &\\[2pt]
  \includegraphics[page=5,width=0.05\textwidth]{imagenes/forkingPath} &  $3 \times 1 \times 3 = 9$ &$9/64$&$1/5$ &$\frac{9}{64}\frac{1}{5}$&\\[2pt]
  \includegraphics[page=6,width=0.05\textwidth]{imagenes/forkingPath} &  $4 \times 0 \times 4 = 0$ &$0/64$&$1/5$ &$\frac{0}{64}\frac{1}{5}$&\\[2pt] \cline{5-5} \\[-0.2cm]
 & & & & $\frac{3 + 8 + 9 }{64 \cdot 5} $&
\end{tabular}
\end{table}
\end{textblock}

}


\only<18>{
\begin{textblock}{128}(0,67)
 \centering \scriptsize
 
 \begin{table}[H]
 \tiny 
\begin{tabular}{cccccc}
\ \ \ Belief \ \ \ &  Ways to produce  \includegraphics[page=1,width=0.045\textwidth]{imagenes/forkingPath}  
& Likelihood & Prior & Posterior $\propto$ & Posterior
\\ \cline{1-6} \\[-0.2cm]

  \includegraphics[page=2,width=0.05\textwidth]{imagenes/forkingPath} &  $0 \times 4 \times 0 = 0$ 
 & $\frac{0 \times 4 \times 0 }{4 \times 4 \times 4 } = \frac{0}{64} $  & $1/5$ & $\frac{0}{64}\frac{1}{5}$ & $\frac{0}{3+8+9} =  0.00 $ 
 \\[2pt]
  \includegraphics[page=3,width=0.05\textwidth]{imagenes/forkingPath} &  $1 \times 3 \times 1 = 3$ 
 &$ 3/64 $ & $1/5$ & $\frac{3}{64}\frac{1}{5}$&$\frac{3}{3+8+9} = 0.15$\\[2pt]
  \includegraphics[page=4,width=0.05\textwidth]{imagenes/forkingPath} &  $2 \times 2 \times 2 = 8$ &$8/64$&$1/5$ &$\frac{8}{64}\frac{1}{5}$ & $\frac{8}{3+8+9}=0.40$\\[2pt]
  \includegraphics[page=5,width=0.05\textwidth]{imagenes/forkingPath} &  $3 \times 1 \times 3 = 9$ &$9/64$&$1/5$ &$\frac{9}{64}\frac{1}{5}$&$\frac{9}{3+8+9}=0.45$\\[2pt]
  \includegraphics[page=6,width=0.05\textwidth]{imagenes/forkingPath} &  $4 \times 0 \times 4 = 0$ &$0/64$&$1/5$ &$\frac{0}{64}\frac{1}{5}$&$\frac{0}{3+8+9}=0.00$\\[2pt] \cline{5-5} \\[-0.2cm]
 & & & & $\frac{3 + 8 + 9 }{64 \cdot 5} $ & 
\end{tabular}
\end{table}
\end{textblock}

}

\end{frame}

\subsection{The garden of (continuous) forking paths }

\begin{frame}
 \begin{textblock}{128}(0,8)
\begin{center}
 \large The garden of (continuous) forking paths 
\end{center}
\end{textblock}
\vspace{1.25cm}


\centering{How to estimate skill of players?}


  \begin{figure}[H]     
     \centering \normalsize
     \begin{subfigure}[b]{0.4\textwidth}
       \includegraphics[page=1,width=\textwidth]{../../Imagenes/elo.jpeg} 
       \caption*{Arpad Elo}
     \end{subfigure}
\end{figure}

\end{frame}

\begin{frame}
 \begin{textblock}{128}(0,8)
\begin{center}
 \normalsize Bayesian Elo factor graph
\end{center}
\end{textblock}
\vspace{0.6cm}

\centering
\includegraphics[width=0.85\textwidth]{imagenes/elo-factor-en}   

\pause
\vspace{0.3cm}

\tiny
The factor graphs specifies the way to compute the posterior, likelihood, and evidence.

\href{https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=910572}{Kschischang FR, Frey BJ, Loeliger HA. Factor graphs and the sum-product algorithm. 2001}

\end{frame}

\begin{frame}
%  \begin{textblock}{128}(0,8)
% \begin{center}
%  \normalsize Posterior
% \end{center}
% \end{textblock}
% \vspace{0.5cm}


\begin{textblock}{128}(0,12)
\only<-6>{
\begin{equation*}
\overbrace{P(s_a \mid r_{ab}, \text{Elo model})}^{\text{Posterior}} \propto \overbrace{N(s_a \, | \, \widehat{\mu}_a, \widehat{\sigma}_a^2) }^{\text{Prior}} \, \overbrace{1-\Phi(s_a \, | \widehat{\mu}_b ,  2\beta^2 + \widehat{\sigma}_b^2)}^{\text{Likelihood}}  \ \ \  \ \ \text{Win case} 
\end{equation*}
}
\end{textblock}
\begin{textblock}{128}(0,12)
\only<7->{
\begin{equation*}
\overbrace{P(s_a \mid r_{ab}, \text{Elo model})}^{\text{Posterior}} \propto \overbrace{N(s_a \, | \, \widehat{\mu}_a, \widehat{\sigma}_a^2) }^{\text{Prior}} \, \overbrace{\Phi(s_a \, | \widehat{\mu}_b ,  2\beta^2 + \widehat{\sigma}_b^2)}^{\text{Likelihood}}  \ \ \  \ \ \text{Loose case} 
\end{equation*}
}
\end{textblock}



\begin{textblock}{128}(0,26)
\centering
\only<2>{\includegraphics[width=0.49\textwidth]{imagenes/posterior_win} 
}
\end{textblock}

\begin{textblock}{128}(0,26)
\centering
\only<3>{\includegraphics[page=2,width=0.49\textwidth]{imagenes/posterior_win} 
}
\end{textblock}

\begin{textblock}{128}(0,26)
\centering
\only<4>{\includegraphics[page=3,width=0.49\textwidth]{imagenes/posterior_win} 
}
\end{textblock}

\begin{textblock}{128}(0,26)
\centering
\only<5>{\includegraphics[page=4,width=0.49\textwidth]{imagenes/posterior_win} 
}
\end{textblock}


\begin{textblock}{128}(0,26)
\centering
\only<6>{\includegraphics[page=5,width=0.49\textwidth]{imagenes/posterior_win} 
}
\end{textblock}

\begin{textblock}{128}(0,26)
\centering
\only<7->{\includegraphics[width=0.49\textwidth]{imagenes/posterior_loose} 
}
\end{textblock}


\begin{textblock}{128}(0,93)
\centering
\only<8>{\tiny For a detailed demostration, see \href{https://journals.plos.org/plosone/article/file?type=supplementary&id=info:doi/10.1371/journal.pone.0211014.s002}{Landfried. TrueSkill: Technical Report. 2019} }
\end{textblock}

%\includegraphics[width=0.49\textwidth]{imagenes/posterior_loose}   
\end{frame}

\section{Bayesian model inference}

\begin{frame}
\begin{textblock}{128}(0,8)
\begin{center}
 \Large Bayesian model inference
\end{center}
\end{textblock}
\vspace{1cm}

$\bullet$ Which are our beliefs about different hidden models $M$?
\pause
\begin{equation*}
 P(M_k|D) = \frac{P(D|M_k)P(M_k)}{\sum_{i=1}^n P(D|M_i) P(M_i)}
\end{equation*}

 \vspace{0.2cm}
\pause
$\bullet$To compare models we can compute their ratio probability,
\pause
\begin{equation*}
\begin{split}
 \text{Bayes factor}(q,r) = \frac{P(M_q|D)}{P(M_r|D)} & = \frac{P(D|M_q)P(M_q)}{P(D|M_r)P(M_r)} \\ \onslide<5->{
 & \overset{*}{=} \frac{P(D|M_q)}{\underbrace{P(D|M_r)}_{\text{\scriptsize Evidence!}}} }
\end{split}
\end{equation*}

\begin{textblock}{128}(96,55)
\onslide<5->{\tiny $* \ \hfrac{\text{With no prior preferences}}{P(M_q) = P(M_r)}$   }
\end{textblock}



\pause \pause

\begin{mdframed}[backgroundcolor=black!20]
\centering
$P(M_q|D) > P(M_r|D) \overset{*}{\Longleftrightarrow} P(D|M_q) > P(D|M_r)$

\end{mdframed}

\pause

\begin{center}
 \large All you need is evidence
\end{center} 


\only<8>{
\begin{textblock}{128}(0,92)
\centering \tiny For a dicussion of bayes factor see \href{http://xyala.cap.ed.ac.uk/teaching/tutorials/phylogenetics/Bayesian_Workshop/PDFs/Kass\%20and\%20Raftery\%201995.pdf}{Kass \& Raftery. Bayes factors. 1995.}
\end{textblock}
}

 
\end{frame}


\subsection{Evidence}


\begin{frame}
\begin{textblock}{128}(0,8)
\begin{center}
 \large \ \ \ \  Evidence 
\end{center}
\end{textblock}
\vspace{1cm}
 
 \centering{
 \includegraphics[width=0.9\textwidth]{imagenes/evidence}
 }
 
 \pause

 \begin{mdframed}[backgroundcolor=black!15]
\centering
Evience encode a trade-off between complexity and prediction.
%Simple models fit a small fraction of data sets, but they assign higher probability to those data points and vice versa. 
\end{mdframed}
 
\end{frame}


\begin{frame}
\begin{textblock}{128}(0,8)
\begin{center}
 \large Evidence vs maximum likelihood
\end{center}
\end{textblock}
\vspace{0.75cm}

\centering{
 \includegraphics[width=0.8\textwidth]{imagenes/minka.png}
 }


\end{frame}

\begin{frame}
\begin{textblock}{128}(0,8)
\begin{center}
 \large Evidence vs maximum likelihood
\end{center}
\end{textblock}
\vspace{1cm}
\Wider[4em]{

\centering{
 \includegraphics[width=0.49\textwidth]{imagenes/minka_evidence.png}
 \includegraphics[width=0.49\textwidth]{imagenes/minka_likelihood.png}
 }

 \vspace{0.3cm}
\pause

}

\begin{mdframed}[backgroundcolor=black!15]
\centering \normalsize
With evidence there is no need for regularization% It automatically encodes a preference for simpler, more constrained models.
\end{mdframed}

\only<3>{
\begin{textblock}{128}(0,92)
\centering \tiny For more examples see \href{http://alumni.media.mit.edu/~tpminka/statlearn/demo/}{Tom Minka} 
\end{textblock}}

\end{frame}


% \begin{frame}
% \begin{textblock}{128}(0,8)
% \begin{center}
%  \large Evidence vs data science metrics
% \end{center}
% \end{textblock}
% \vspace{0.75cm}
% 
% 
% \Wider[1cm]{
% \begin{center}
%  Which model has highest probability given the observed data $D$
%  \end{center}
% }
% 
% 
%  \begin{table}[H]
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
% \hline
%  Ana & 0.6 & 0.4 & 0.4 & 0.4 & 0.6 & 0.6 & 0.6 & 0.6 & 0.6 & 0.6 \\ 
% \hline \hline
%  Observed& \includegraphics[width=0.02\textwidth]{imagenes/sol}  & \includegraphics[width=0.02\textwidth]{imagenes/lluvia} & \includegraphics[width=0.02\textwidth]{imagenes/lluvia} & \includegraphics[width=0.02\textwidth]{imagenes/lluvia} & \includegraphics[width=0.02\textwidth]{imagenes/sol} & \includegraphics[width=0.02\textwidth]{imagenes/sol} & \includegraphics[width=0.02\textwidth]{imagenes/sol} & \includegraphics[width=0.02\textwidth]{imagenes/sol} & \includegraphics[width=0.02\textwidth]{imagenes/sol} & \includegraphics[width=0.02\textwidth]{imagenes/sol} \\
% \hline \hline
%  Berta & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0   \\
%  \hline
% \end{tabular}
% \end{table}
% 
% \pause
% 
% \Wider[1cm]{
% \begin{itemize}
%  \item[$\bullet$] Berta seems to do a better job according to almost all data science metrics: 
% 
%  \emph{MSE, MAE, Hit Rate, Accuracy, Precision, Recall, F-Score, ROC Area}
%  \pause
%  \item[$\bullet$] However Ana's model assigns highest probability to the observed! 
%  
%  \textbf{\emph{Evidence, Cross Entropy}}
% \end{itemize}
% }
% 
% \pause \vspace{0.2cm}
% 
%   \begin{table}[H]
% \begin{tabular}{c|c}
%  & $P(D|M) = \prod P(D_i|M)$ \\ \hline
%  Ana &  $0.4^{10}= 10.1E^{-5}$  \\
% Berta & $1.0^8  \times 0^2 = 0 = 1E^{-\infty}$\\
%  \end{tabular}
% \end{table}
% \vspace{-0.1cm}
% \begin{framed}
% \centering 
% 
% Ana has an infinitely more likely model
% \end{framed}
% 
% 
% \end{frame}


% 
% \subsection{Metricas}
% 
% \begin{frame}
% \begin{table}[H]
% \centering
% \begin{tabular}{rlll}
%   \hline
%  & Ana & Berta & Gana \\ 
%   \hline
% Mean squared error & 3.6 & 2 & Berta \\ 
%   Mean absolute error & 6 & 2 & Berta \\ 
%   Hit rate & 0.4 & 0.8 & Berta \\ 
%   Accuracy & 0 & 0.8 & Berta \\ 
%   Precsion & 0 & 0.67 & Berta \\ 
%   Recall & NaN & 0.67 & Berta \\ 
%   F1 score & NaN & 0.25 & Berta \\ 
%   ROC area & 0 & 0.76 & Berta \\ 
%   Evidence & $10.4E^{-5}$ & 0 & Ana \\ 
%    \hline
% \end{tabular}
% \end{table}
% 
% 
% \end{frame}
% 


\begin{frame}
 
 \vspace{1cm}
 
 \begin{center}
  \huge
  Why evidence is not widely used in machine learning?
 \end{center}

 \vspace{1cm}
\pause
 
 \begin{framed}
  \centering \large First let's take a look at Bayesian no-doubt case
 \end{framed}

  
 
 
\end{frame}



\section{Bayesian no-doubt case}

\begin{frame}
 \begin{textblock}{128}(0,8)
\begin{center}
 \Large Bayesian no-doubt case
\end{center}
\end{textblock}
\vspace{0.5cm}


 \centering{Fixed beliefs, even with infinite new data}
 \begin{mdframed}[backgroundcolor=black!15]
 \centering
 $\#$Beliefs = 1 $\Longrightarrow$ $\underbrace{P(B)}_{\text{\scriptsize Prior}} = \underbrace{P(B|D)}_{\text{\scriptsize Posterior}} \ \ \ \ \hfrac{\forall D \in \text{Data}}{\forall B \in \text{Beliefs}}$
\end{mdframed}

 \vspace{0.9cm}
\pause 


 \centering{Likelihood is just the Evidence}
\begin{mdframed}[backgroundcolor=black!15]
\centering
 $\#$Beliefs = 1  $\Longleftrightarrow$ Likelihood = Evidence  
\end{mdframed}
 

 

\end{frame}

\begin{frame}
 \vspace{0.5cm}


\begin{center}
 \centering \large
  Who has no doubt? Who has only one belief?
\end{center}
 
 \vspace{0.3cm}
 \pause
 
 \begin{itemize}
  \item[$\bullet$] God (if exists) \pause
  \item[$\bullet$] Mathematicians (and other non-empricial sciences) \pause
  \item[$\bullet$] Maybe some extremists \pause
  \item[$\bullet$] \textbf{All non-bayesian machine learning} \pause (the hacked-belief approach)
 \end{itemize}

 
\end{frame}




\subsection{The hacked-belief approach}


\begin{frame}
 \begin{textblock}{128}(0,8)
\begin{center}
 \large The hacked-belief approach
\end{center}
\end{textblock}
\vspace{0.5cm}

\begin{center}
 \centering  $ \overset{\text{The best belief after seeing the data}}{\text{\st{maximum likelihood estimator} }}$ = $\underset{B }{\text{argmax}} P(D|B,M) = \widehat{B}$  
\end{center}


\pause

 \begin{equation*}
\overbrace{P(D|M)}^{\text{\scriptsize Hacked \textbf{evidence}}} = \ \ \overbrace{P(D|\widehat{B},M)}^{\text{\scriptsize Hacked \textbf{likelihood} }} \pause  = \  \underset{\text{Data appears back and forth!!}}{\overbrace{P(\underset{\uparrow}{D}|\underset{B}{\text{argmax}} P(\underset{\uparrow}{D}|B,M), M)}^{\text{\scriptsize Maximum likelihood}}}
\end{equation*}


\pause

\vspace{0.3cm}

\begin{mdframed}[backgroundcolor=black!15]
\centering
 \large Hacked evidence (with MLE) = Maximum likelihood
\end{mdframed}

\pause

\vspace{0.3cm}
\begin{center}
 \centering \large With great hacked-belief approach comes great overfitting!  
\end{center}
  
 
\end{frame}


\begin{frame}
 \begin{textblock}{128}(0,8)
\begin{center}
\normalsize With great overfitting comes great regularization!
\end{center}
\end{textblock}



\begin{center}
 \centering  $ \overset{\text{The best belief after seeing the data}}{\text{\st{maximum a posteriori (estimator)} }}$ = $\underset{B }{\text{argmax}} P(D|B,M) P(B|M) = \widehat{B}$  
\end{center}


\pause

 \begin{equation*}
\overbrace{P(D|M)}^{\text{\scriptsize Hacked \textbf{evidence}}} = \ \ \overbrace{P(D|\widehat{B},M)}^{\text{\scriptsize Hacked \textbf{likelihood} }} \pause  = \  \underset{\text{Again data appears back and forth!!}}{\overbrace{P(\underset{\uparrow}{D}|\underset{B}{\text{argmax}} P(\underset{\uparrow}{D}|B,M)P(B|M),M)}^{\text{\scriptsize Likelihood at maximum a posteriori}}}
\end{equation*}

\pause
\vspace{0.4cm}

\Wider[4em]{
\begin{framed}
\normalsize \centering
Hacked evidence (with MAP) = L2 or L1 regularization
\end{framed}
}

\pause 

 \only<5->{
\begin{textblock}{128}(0,91)
\centering
 \tiny For more regularization techniques for hacked-belief approach see 
 
 \href{http://bit.ly/slides_regularizacion}{Zivik talk. With great complexity comes great regularization}  
\end{textblock}
}
 
\end{frame}


\begin{frame}
 \begin{textblock}{128}(0,8)
\begin{center}
 \large Evidence and data science metrics
\end{center}
\end{textblock}
\vspace{0.5cm}

\begin{center}
 \normalsize Is there any data science metrics equivalent to evidence?
\end{center}


\begin{equation*}
 \begin{split}
\onslide<2->{\overbrace{P(D|M)}^{\text{Evidence}} }
\onslide<3->{\propto \text{log} \, P(D|M) }
&\onslide<4->{= \text{log} \left( \prod_{i=1}^{|D|} P(D_i|M) \right) }
\onslide<5->{=  \sum_{i=1}^{|D|} \text{log} \, P(D_i|M)} \\
&\onslide<6->{\propto  \frac{1}{|D|} \sum_{i=1}^{|D|} \text{log} P(D_i|M)}
\onslide<7->{= E_p \left[ \text{log} \, P(D|M) \right] }  \\[2pt]
& \onslide<8->{= E_p \left[ \text{log} \, q \right] }  
\onslide<9->{= - \underbrace{H(p,q)}_{\text{Cross entropy}} } \\
\end{split}
\end{equation*}

\onslide<10->{
\begin{mdframed}[backgroundcolor=black!15]
\centering \normalsize
 Evidence $\propto$ Cross entropy \only<11->{(at validation data set)}
\end{mdframed}
}

 
\end{frame}


% 
% 
% 
% \begin{frame}
%  
%  \begin{center}
%   \Large Annex
%  \end{center}
% 
%  
% \end{frame}
% 
% 
% \begin{frame}
% \begin{textblock}{128}(0,8)
% \begin{center}
% Annex 1 \\ 
%  \scriptsize Proof: Deviance -- Bayes factor
% \end{center}
% \end{textblock}
% \vspace{0.5cm}
% 
% 
% \tiny
% 
% % 
% Entropy of a probability distribution
% \begin{equation}
%  H(P(C)) = -E(log(P(C))) = - \sum_{i=1}^n P(C_i) log(P(C_i))
% \end{equation}
% 
% \pause
% \vspace{0.3cm}
% 
% $\bullet$ The log Bayes factor is essentially the relative distance to the truth
% 
% \begin{mdframed}[backgroundcolor=black!15]
% \begin{equation}\footnotesize
%   D_{KL}(p,q) - D_{KL}(p,r) \overset{\text{Annex 1}}{=} \frac{1}{|D|} \text{log}\left(\text{Bayes factor}(r,q)\right) 
% \end{equation}
% \end{mdframed}
% 
% 
% 
% \begin{equation}
%  D_{KL}(p,q) = \sum_{i=1}^{|C|} p_i  (\text{log}(p_i) - \text{log}(q_i))
% \end{equation}
% With truth $P(C_i)=p_i$ and model $P(C_i|M_q)=q_i$, for each event $C_i \in C$%, and
% 
% 
% 
% 
% \begin{equation}
% \text{Deviance}(q,r) = D_{KL}(p,q) - D_{KL}(p,r) = \underbrace{\sum_{i=1}^{|C|} p_i \text{log}(r_i)}_{E[\text{log}(r)]} -\underbrace{\sum_{i=1}^{|C|} p_i \text{log}(q_i)}_{E[\text{log}(q)]}   
%  \end{equation}
% 
%  
%  \vspace{0.3cm}
%  
%  Where
%  \begin{equation*}
% \begin{align*}
%  E[\text{log}(q)] & = \sum_{i=1}^{|C|} P(C_i) \text{log}(P(C_i|M_q))  \approx  \frac{1}{|D|} \sum_{i=1}^{|D|} \text{log}(P(D_i|M_q)) \\
%  & = \frac{1}{|D|} \text{log}\left(\prod_{i=1}^{|D|} P(D_i|M_q)\right)= \frac{1}{|D|} \text{log}\underbrace{(P(D|M_q))}_{\text{Evidence!}}
% \end{align*}
% \end{equation*}
% 
% \vspace{0.3cm}
% 
% Then
% \begin{equation*}
% \begin{align*}
% D_{KL}(p,q) - D_{KL}(p,r) & \approx \frac{1}{|D|} \text{log}(P(D|M_q)) - \frac{1}{|D|} \text{log}(P(D|M_q)) \\
% & = \frac{1}{|D|} \text{log}\left(\frac{P(D|M_r)}{P(D|M_q)} \right) \\
% & = \frac{1}{|D|} \text{log}\left(\text{Bayes factor}\right) 
% \end{align*}
%  \end{equation*}
%  
% \end{frame}

\end{document}



